---
title: 'Insert Data'
openapi: 'POST /v1/blockchain/{chain}/{table_name}/insert'
---

The `Insert` endpoint allows you to push your raw blockchain data onto Dune. Supported tables and required schemas are listed below. 

On the initial insert, the endpoint will automatically create a table for you under the associated namespace under your API key, e.g. `my_chain`.`blocks`.

## Supported tables and required schemas 

<AccordionGroup>
  <Accordion title="blocks">
    | Name | Type | Nullable |
    |------|------|----------|
    | date | date | false |
    | time | timestamp | |
    | number | bigint | |
    | gas_limit | uint256 | |
    | gas_used | uint256 | |
    | difficulty | uint256 | |
    | total_difficulty | uint256 | |
    | size | bigint | |
    | base_fee_per_gas | bigint | |
    | state_root | varbinary | |
    | transactions_root | varbinary | |
    | receipts_root | varbinary | |
    | hash | varbinary | |
    | parent_hash | varbinary | |
    | miner | varbinary | |
    | nonce | varbinary | |
    | extra_data | varbinary | |
    | l1_batch_number | bigint | |
    | l1_batch_timestamp | timestamp | |
  </Accordion>
  <Accordion title="transactions">
    | Name | Type | Nullable |
    |------|------|----------|
    | block_date | date | false |
    | block_time | timestamp | |
    | value | uint256 | |
    | block_number | bigint | |
    | gas_limit | uint256 | |
    | gas_price | uint256 | |
    | gas_used | uint256 | |
    | max_fee_per_gas | bigint | |
    | max_priority_fee_per_gas | bigint | |
    | priority_fee_per_gas | bigint | |
    | nonce | bigint | |
    | index | bigint | |
    | success | boolean | |
    | type | varchar | |
    | chain_id | varchar | |
    | from | varbinary | |
    | to | varbinary | |
    | block_hash | varbinary | |
    | data | varbinary | |
    | hash | varbinary | |
    | l1_gas_used | bigint | |
    | l1_gas_price | bigint | |
    | l1_fee | bigint | |
    | l1_fee_scalar | double | |
    | l1_block_number | bigint | |
    | l1_timestamp | bigint | |
    | l1_tx_origin | varbinary | |
  </Accordion>
  <Accordion title="logs">
    | Name | Type | Nullable |
    |------|------|----------|
    | block_date | date | false |
    | block_time | timestamp | |
    | block_number | bigint | |
    | block_hash | varbinary | |
    | contract_address | varbinary | |
    | topic0 | varbinary | |
    | topic1 | varbinary | |
    | topic2 | varbinary | |
    | topic3 | varbinary | |
    | data | varbinary | |
    | tx_hash | varbinary | |
    | index | integer | |
    | tx_index | integer | |
    | tx_from | varbinary | |
    | tx_to | varbinary | |
    | l1_batch_number | bigint | |
    | l1_batch_tx_index | bigint | |
  </Accordion>
  <Accordion title="traces">
    | Name | Type | Nullable |
    |------|------|----------|
    | block_date | date | false |
    | block_time | timestamp | |
    | block_number | bigint | |
    | value | uint256 | |
    | gas | uint256 | |
    | gas_used | uint256 | |
    | call_type | varchar | |
    | success | boolean | |
    | revert_reason | varchar | |
    | error | varchar | |
    | tx_success | boolean | |
    | type | varchar | |
    | block_hash | varbinary | |
    | tx_index | integer | |
    | sub_traces | bigint | |
    | tx_hash | varbinary | |
    | from | varbinary | |
    | to | varbinary | |
    | trace_address | array[bigint] | |
    | address | varbinary | |
    | code | varbinary | |
    | input | varbinary | |
    | output | varbinary | |
    | refund_address | varbinary | |
    | tx_from | varbinary | |
    | tx_to | varbinary | |
  </Accordion>
</AccordionGroup>

<Note>
- Ensure that data in the files strictly conforms to the specified schema and uses identical column names.
- For columns of type `varbinary`, submit the data in string format. See [varbinary values](../endpoint/insert#varbinary-values) for more info.
- The maximum allowable request size is 1.2GB.
</Note>

## Consistency
A request to this endpoint has two possible outcomes: Either all of the data in the request was inserted, or none of it was.
It's not possible for parts of the request data to be inserted, while the rest is not inserted.
In other words, please use and trust the status codes that the endpoint returns.
A status code of 200 means that the data in the request was successfully inserted.
If you get any other status code, you can safely retry your request after addressing the issue that the error message indicated.

## Concurrent requests
A limited number of concurrent insertion requests per table is supported. However, there will be a slight performance penalty as we serialize the writes behind the scenes to ensure data integrity. Larger number of concurrent requests per table may result in an increased number of failures. Therefore, we recommend managing your requests within a 5-10 threshold to maintain optimal performance.

## Supported filetypes
### CSV files (`Content-Type: text/csv`)
CSV files must use a comma as delimiter, and the column names in the header must match the column names of the table.

### JSON files (`Content-Type: application/x-ndjson`)
These are files where each line is a complete JSON object which creates one table row.
Each line must have keys that match the column names of the table.


<RequestExample>
```bash curl
curl --request POST \
  --url https://api.dune.com/api/v1/blockchain/my_chain/blocks/insert \
  --header 'X-DUNE-API-KEY: <x-dune-api-key>' \
  --header 'Content-Type: text/csv' \
  --upload-file blocks.csv
```


```python Python
import requests


url = "https://api.dune.com/api/v1/blockchain/my_chain/blocks/insert"


headers = {
   "X-DUNE-API-KEY": "{api_key}", 
   "Content-Type": "text/csv"
}


with open("./blocks.csv", "rb") as data:
 response = requests.request("POST", url, data=data, headers=headers)
```

</RequestExample>
